{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e441c5-d9f3-4d9c-af18-e9588eaac7d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/sabo/Bench/hisse_tahmin/notebooks']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3303\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3300\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile \u001b[38;5;28;01mif\u001b[39;00m shell_futures \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_class()\n\u001b[1;32m   3302\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3303\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3305\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3306\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3307\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/ipykernel/compiler.py:105\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/ipykernel/compiler.py:91\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 91\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/tempfile.py:315\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgettempdir\u001b[39m():\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns tempfile.tempdir as str.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mfsdecode(\u001b[43m_gettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/tempfile.py:308\u001b[0m, in \u001b[0;36m_gettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/sabo/Bench/hisse_tahmin/notebooks']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATA_DIR = '../data'  \n",
    "MODEL_DIR = '../models'\n",
    "MIN_DATA_POINTS = 252 \n",
    "\n",
    "SP500_TICKER_FILE = os.path.join(DATA_DIR, 'sp500_tickers_correct.csv')\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def get_sp500_tickers_from_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Symbol' not in df.columns:\n",
    "            print(f\"HATA: '{file_path}' dosyasında 'Symbol' sütunu bulunamadı.\")\n",
    "            return None\n",
    "        \n",
    "        tickers = df['Symbol'].tolist()\n",
    "        print(f\"'{file_path}' dosyasından {len(tickers)} adet S&P 500 hisse senedi sembolü başarıyla okundu.\")\n",
    "        return tickers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"HATA: S&P 500 ticker dosyası bulunamadı: '{file_path}'\")\n",
    "        print(\"Lütfen önce 'scrape_sp500.py' dosyasını çalıştırdığınızdan emin olun.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: S&P 500 listesi okunurken bir hata oluştu: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfa0af-b0ea-4e79-9dc9-ce18cd71fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        stock_ticker = os.path.basename(file_path).split('.')[0]\n",
    "        print(f\"--- {stock_ticker} verisi işleniyor... ---\")\n",
    "\n",
    "        required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            print(f\"HATA: {stock_ticker}.csv dosyasında gerekli sütunlar bulunamadı. Atlanıyor.\")\n",
    "            return None, None\n",
    "\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        df.dropna(how='any', inplace=True)\n",
    "\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        df = df[(df[price_cols] > 0).all(axis=1)]\n",
    "\n",
    "        if len(df) < MIN_DATA_POINTS:\n",
    "            print(f\"UYARI: {stock_ticker} için yeterli veri yok ({len(df)} satır). Atlanıyor.\")\n",
    "            return None, None\n",
    "        \n",
    "        return df, stock_ticker\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: {file_path} işlenirken bir hata oluştu: {e}. Atlanıyor.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a50f9-e1aa-4a4c-8d11-1bad23cbb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "    delta = df['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['ATR'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    df['Target'] = (df['Close'] > df['Open']).astype(int)\n",
    "    df['Target'] = df['Target'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    df['Target'] = df['Target'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba97ab-34c6-40ec-af07-f880ca1bb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_dataset(X, y, window_size=5):\n",
    "    X_windowed, y_windowed = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        features = X.iloc[i-window_size:i].values.flatten()\n",
    "        X_windowed.append(features)\n",
    "        y_windowed.append(y.iloc[i])\n",
    "    return np.array(X_windowed), np.array(y_windowed)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_experiments_for_stock(df, stock_ticker):\n",
    "    X_full = df.drop('Target', axis=1)\n",
    "    y_full = df['Target']\n",
    "    stock_results = []\n",
    "    \n",
    "    feature_subsets = {\n",
    "        'Tum_Ozellikler': X_full.columns.tolist(),\n",
    "        'Sadece_Teknik_Indikatorler': ['EMA_10', 'EMA_20', 'EMA_50', 'RSI', 'ATR'],\n",
    "    }\n",
    "    \n",
    "    k_values = [5, 10, 15] \n",
    "    window_sizes = [3, 7]\n",
    "    max_depth_values = [10, 20, None]\n",
    "    split_ratio = 0.8\n",
    "\n",
    "    for subset_name, subset_columns in feature_subsets.items():\n",
    "        X_subset = X_full[subset_columns]\n",
    "        for k in k_values:\n",
    "            current_k = min(k, X_subset.shape[1])\n",
    "            if k > current_k:\n",
    "                print(f\"\\nAtlanıyor: İstenen k={k}, mevcut özellik sayısından ({current_k}) fazla.\")\n",
    "                continue\n",
    "\n",
    "            for window in window_sizes:\n",
    "                for depth in max_depth_values:\n",
    "                    print(f\"\\n--- Deney: {subset_name} | k={current_k} | Pencere={window} | max_depth={depth} ---\")\n",
    "\n",
    "                    selector = SelectKBest(f_classif, k=current_k).fit(X_subset, y_full)\n",
    "                    X_selected = X_subset[X_subset.columns[selector.get_support()]]\n",
    "                    \n",
    "                    num_selected_features = X_selected.shape[1]\n",
    "                    print(f\"SelectKBest sonrası seçilen özellik sayısı: {num_selected_features}\")\n",
    "\n",
    "                    X_windowed, y_windowed = create_windowed_dataset(X_selected, y_full, window_size=window)\n",
    "                    if len(X_windowed) == 0: continue\n",
    "                    \n",
    "                    final_feature_count = X_windowed.shape[1]\n",
    "                    print(f\"Pencereleme sonrası nihai özellik sayısı: {final_feature_count}\")\n",
    "                    \n",
    "                    split_index = int(len(X_windowed) * split_ratio)\n",
    "                    X_train_w, X_test_w = X_windowed[:split_index], X_windowed[split_index:]\n",
    "                    y_train_w, y_test_w = y_windowed[:split_index], y_windowed[split_index:]\n",
    "                    \n",
    "                    scaler = StandardScaler().fit(X_train_w)\n",
    "                    X_train_scaled = scaler.transform(X_train_w)\n",
    "                    X_test_scaled = scaler.transform(X_test_w)\n",
    "                    \n",
    "                    model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1)\n",
    "                    model.fit(X_train_scaled, y_train_w)\n",
    "                    \n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    accuracy = accuracy_score(y_test_w, y_pred)\n",
    "                    \n",
    "                    model_filename = os.path.join(MODEL_DIR, f\"rf_{stock_ticker}_subset-{subset_name}_k{current_k}_w{window}_d{depth}.joblib\")\n",
    "                    joblib.dump(model, model_filename)\n",
    "                    \n",
    "                    stock_results.append({\n",
    "                        'model_tipi': 'RandomForest',\n",
    "                        'hisse_senedi': stock_ticker,\n",
    "                        'alt_kume': subset_name,\n",
    "                        'k_ozellik_sayisi': current_k,\n",
    "                        'pencere_boyutu': window,\n",
    "                        'max_depth': depth,\n",
    "                        'dogruluk': accuracy,\n",
    "                        'model_dosyasi': model_filename\n",
    "                    })\n",
    "    \n",
    "    print(f\"==> {stock_ticker} için {len(stock_results)} RandomForest deneyi tamamlandı.\")\n",
    "    return stock_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0b3fe-f753-4524-82b8-53bccdbe07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sp500_tickers = get_sp500_tickers_from_csv(SP500_TICKER_FILE)\n",
    "    if sp500_tickers is None:\n",
    "        return \n",
    "    \n",
    "    all_csv_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
    "    if not all_csv_files:\n",
    "        print(f\"UYARI: '{DATA_DIR}' klasöründe hiçbir .csv dosyası bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    target_csv_files = []\n",
    "    sp500_set = set(sp500_tickers)\n",
    "    for file_path in all_csv_files:\n",
    "        ticker = os.path.basename(file_path).split('.')[0]\n",
    "        if ticker in sp500_set:\n",
    "            target_csv_files.append(file_path)\n",
    "            \n",
    "    print(f\"\\nİşlenecek {len(target_csv_files)} adet S&P 500 hissesi bulundu.\")\n",
    "\n",
    "    all_results = []\n",
    "    for file_path in target_csv_files:\n",
    "        df_clean, stock_ticker = load_and_clean_data(file_path)\n",
    "        if df_clean is None:\n",
    "            continue\n",
    "            \n",
    "        df_featured = feature_engineering(df_clean)\n",
    "        if len(df_featured) < MIN_DATA_POINTS:\n",
    "            print(f\"UYARI: {stock_ticker} için özellik mühendisliği sonrası yeterli veri kalmadı. Atlanıyor.\")\n",
    "            continue\n",
    "        \n",
    "        results_for_one_stock = run_experiments_for_stock(df_featured, stock_ticker)\n",
    "        all_results.extend(results_for_one_stock)\n",
    "\n",
    "    print(\"\\n--- TÜM HİSSE SENETLERİ İÇİN TÜM DENEYLER TAMAMLANDI ---\")\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"Hiçbir hisse senedi için başarılı bir deney sonucu elde edilemedi.\")\n",
    "        return\n",
    "        \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df_sorted = results_df.sort_values(by=['hisse_senedi', 'dogruluk'], ascending=[True, False])\n",
    "\n",
    "    print(\"\\nBirleştirilmiş Deney Sonuçları Tablosu:\")\n",
    "    display(results_df_sorted)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a9cf7-3060-4710-a0a4-0b51b6ca5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu disk spacim dolduğu için en sonda bir hata aldım vaktim yetmedi disk spaceini temizlemeye ve yeniden runlamaya "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
