{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e441c5-d9f3-4d9c-af18-e9588eaac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATA_DIR = '../data'  \n",
    "MODEL_DIR = '../models'\n",
    "MIN_DATA_POINTS = 252 \n",
    "\n",
    "SP500_TICKER_FILE = os.path.join(DATA_DIR, 'sp500_tickers_correct.csv')\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def get_sp500_tickers_from_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Symbol' not in df.columns:\n",
    "            print(f\"HATA: '{file_path}' dosyasında 'Symbol' sütunu bulunamadı.\")\n",
    "            return None\n",
    "        \n",
    "        tickers = df['Symbol'].tolist()\n",
    "        print(f\"'{file_path}' dosyasından {len(tickers)} adet S&P 500 hisse senedi sembolü başarıyla okundu.\")\n",
    "        return tickers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"HATA: S&P 500 ticker dosyası bulunamadı: '{file_path}'\")\n",
    "        print(\"Lütfen önce 'scrape_sp500.py' dosyasını çalıştırdığınızdan emin olun.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: S&P 500 listesi okunurken bir hata oluştu: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dfa0af-b0ea-4e79-9dc9-ce18cd71fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        stock_ticker = os.path.basename(file_path).split('.')[0]\n",
    "        print(f\"--- {stock_ticker} verisi işleniyor... ---\")\n",
    "\n",
    "        required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            print(f\"HATA: {stock_ticker}.csv dosyasında gerekli sütunlar bulunamadı. Atlanıyor.\")\n",
    "            return None, None\n",
    "\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        df.dropna(how='any', inplace=True)\n",
    "\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        df = df[(df[price_cols] > 0).all(axis=1)]\n",
    "\n",
    "        if len(df) < MIN_DATA_POINTS:\n",
    "            print(f\"UYARI: {stock_ticker} için yeterli veri yok ({len(df)} satır). Atlanıyor.\")\n",
    "            return None, None\n",
    "        \n",
    "        return df, stock_ticker\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: {file_path} işlenirken bir hata oluştu: {e}. Atlanıyor.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337a50f9-e1aa-4a4c-8d11-1bad23cbb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "    delta = df['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['ATR'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    df['Target'] = (df['Close'] > df['Open']).astype(int)\n",
    "    df['Target'] = df['Target'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    df['Target'] = df['Target'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ba97ab-34c6-40ec-af07-f880ca1bb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_dataset(X, y, window_size=5):\n",
    "    X_windowed, y_windowed = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        features = X.iloc[i-window_size:i].values.flatten()\n",
    "        X_windowed.append(features)\n",
    "        y_windowed.append(y.iloc[i])\n",
    "    return np.array(X_windowed), np.array(y_windowed)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_experiments_for_stock(df, stock_ticker):\n",
    "    X_full = df.drop('Target', axis=1)\n",
    "    y_full = df['Target']\n",
    "    stock_results = []\n",
    "    \n",
    "    feature_subsets = {\n",
    "        'Tum_Ozellikler': X_full.columns.tolist(),\n",
    "        'Sadece_Teknik_Indikatorler': ['EMA_10', 'EMA_20', 'EMA_50', 'RSI', 'ATR'],\n",
    "    }\n",
    "    \n",
    "    k_values = [5, 10, 15] \n",
    "    window_sizes = [3, 7]\n",
    "    max_depth_values = [10, 20, None]\n",
    "    split_ratio = 0.8\n",
    "\n",
    "    for subset_name, subset_columns in feature_subsets.items():\n",
    "        X_subset = X_full[subset_columns]\n",
    "        for k in k_values:\n",
    "            current_k = min(k, X_subset.shape[1])\n",
    "            if k > current_k:\n",
    "                print(f\"\\nAtlanıyor: İstenen k={k}, mevcut özellik sayısından ({current_k}) fazla.\")\n",
    "                continue\n",
    "\n",
    "            for window in window_sizes:\n",
    "                for depth in max_depth_values:\n",
    "                    print(f\"\\n--- Deney: {subset_name} | k={current_k} | Pencere={window} | max_depth={depth} ---\")\n",
    "\n",
    "                    selector = SelectKBest(f_classif, k=current_k).fit(X_subset, y_full)\n",
    "                    X_selected = X_subset[X_subset.columns[selector.get_support()]]\n",
    "                    \n",
    "                    num_selected_features = X_selected.shape[1]\n",
    "                    print(f\"SelectKBest sonrası seçilen özellik sayısı: {num_selected_features}\")\n",
    "\n",
    "                    X_windowed, y_windowed = create_windowed_dataset(X_selected, y_full, window_size=window)\n",
    "                    if len(X_windowed) == 0: continue\n",
    "                    \n",
    "                    final_feature_count = X_windowed.shape[1]\n",
    "                    print(f\"Pencereleme sonrası nihai özellik sayısı: {final_feature_count}\")\n",
    "                    \n",
    "                    split_index = int(len(X_windowed) * split_ratio)\n",
    "                    X_train_w, X_test_w = X_windowed[:split_index], X_windowed[split_index:]\n",
    "                    y_train_w, y_test_w = y_windowed[:split_index], y_windowed[split_index:]\n",
    "                    \n",
    "                    scaler = StandardScaler().fit(X_train_w)\n",
    "                    X_train_scaled = scaler.transform(X_train_w)\n",
    "                    X_test_scaled = scaler.transform(X_test_w)\n",
    "                    \n",
    "                    model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1)\n",
    "                    model.fit(X_train_scaled, y_train_w)\n",
    "                    \n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    accuracy = accuracy_score(y_test_w, y_pred)\n",
    "                    \n",
    "                    model_filename = os.path.join(MODEL_DIR, f\"rf_{stock_ticker}_subset-{subset_name}_k{current_k}_w{window}_d{depth}.joblib\")\n",
    "                    joblib.dump(model, model_filename)\n",
    "                    \n",
    "                    stock_results.append({\n",
    "                        'model_tipi': 'RandomForest',\n",
    "                        'hisse_senedi': stock_ticker,\n",
    "                        'alt_kume': subset_name,\n",
    "                        'k_ozellik_sayisi': current_k,\n",
    "                        'pencere_boyutu': window,\n",
    "                        'max_depth': depth,\n",
    "                        'dogruluk': accuracy,\n",
    "                        'model_dosyasi': model_filename\n",
    "                    })\n",
    "    \n",
    "    print(f\"==> {stock_ticker} için {len(stock_results)} RandomForest deneyi tamamlandı.\")\n",
    "    return stock_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c0b3fe-f753-4524-82b8-53bccdbe07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../data/sp500_tickers_correct.csv' dosyasından 503 adet S&P 500 hisse senedi sembolü başarıyla okundu.\n",
      "\n",
      "İşlenecek 472 adet S&P 500 hissesi bulundu.\n",
      "--- MGM verisi işleniyor... ---\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=3 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=3 | max_depth=20 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=3 | max_depth=None ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=7 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 35\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=7 | max_depth=20 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 35\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=5 | Pencere=7 | max_depth=None ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 35\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=3 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 30\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=3 | max_depth=20 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 30\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=3 | max_depth=None ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 30\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=7 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 70\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=7 | max_depth=20 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 70\n",
      "\n",
      "--- Deney: Tum_Ozellikler | k=10 | Pencere=7 | max_depth=None ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 10\n",
      "Pencereleme sonrası nihai özellik sayısı: 70\n",
      "\n",
      "Atlanıyor: İstenen k=15, mevcut özellik sayısından (11) fazla.\n",
      "\n",
      "--- Deney: Sadece_Teknik_Indikatorler | k=5 | Pencere=3 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Sadece_Teknik_Indikatorler | k=5 | Pencere=3 | max_depth=20 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Sadece_Teknik_Indikatorler | k=5 | Pencere=3 | max_depth=None ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 15\n",
      "\n",
      "--- Deney: Sadece_Teknik_Indikatorler | k=5 | Pencere=7 | max_depth=10 ---\n",
      "SelectKBest sonrası seçilen özellik sayısı: 5\n",
      "Pencereleme sonrası nihai özellik sayısı: 35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     display(results_df_sorted)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUYARI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock_ticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m için özellik mühendisliği sonrası yeterli veri kalmadı. Atlanıyor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     results_for_one_stock \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments_for_stock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_featured\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_ticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(results_for_one_stock)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- TÜM HİSSE SENETLERİ İÇİN TÜM DENEYLER TAMAMLANDI ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mrun_experiments_for_stock\u001b[0;34m(df, stock_ticker)\u001b[0m\n\u001b[1;32m     56\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_w)\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39mdepth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[1;32m     62\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_w, y_pred)\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/sklearn/base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1361\u001b[0m     )\n\u001b[1;32m   1362\u001b[0m ):\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:486\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    475\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    478\u001b[0m ]\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/sklearn/utils/parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[1;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hisse_tahmin/lib/python3.10/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sp500_tickers = get_sp500_tickers_from_csv(SP500_TICKER_FILE)\n",
    "    if sp500_tickers is None:\n",
    "        return \n",
    "    \n",
    "    all_csv_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
    "    if not all_csv_files:\n",
    "        print(f\"UYARI: '{DATA_DIR}' klasöründe hiçbir .csv dosyası bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    target_csv_files = []\n",
    "    sp500_set = set(sp500_tickers)\n",
    "    for file_path in all_csv_files:\n",
    "        ticker = os.path.basename(file_path).split('.')[0]\n",
    "        if ticker in sp500_set:\n",
    "            target_csv_files.append(file_path)\n",
    "            \n",
    "    print(f\"\\nİşlenecek {len(target_csv_files)} adet S&P 500 hissesi bulundu.\")\n",
    "\n",
    "    all_results = []\n",
    "    for file_path in target_csv_files:\n",
    "        df_clean, stock_ticker = load_and_clean_data(file_path)\n",
    "        if df_clean is None:\n",
    "            continue\n",
    "            \n",
    "        df_featured = feature_engineering(df_clean)\n",
    "        if len(df_featured) < MIN_DATA_POINTS:\n",
    "            print(f\"UYARI: {stock_ticker} için özellik mühendisliği sonrası yeterli veri kalmadı. Atlanıyor.\")\n",
    "            continue\n",
    "        \n",
    "        results_for_one_stock = run_experiments_for_stock(df_featured, stock_ticker)\n",
    "        all_results.extend(results_for_one_stock)\n",
    "\n",
    "    print(\"\\n--- TÜM HİSSE SENETLERİ İÇİN TÜM DENEYLER TAMAMLANDI ---\")\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"Hiçbir hisse senedi için başarılı bir deney sonucu elde edilemedi.\")\n",
    "        return\n",
    "        \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df_sorted = results_df.sort_values(by=['hisse_senedi', 'dogruluk'], ascending=[True, False])\n",
    "\n",
    "    print(\"\\nBirleştirilmiş Deney Sonuçları Tablosu:\")\n",
    "    display(results_df_sorted)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a9cf7-3060-4710-a0a4-0b51b6ca5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu disk spacim dolduğu için en sonda bir hata aldım vaktim yetmedi disk spaceini temizlemeye ve yeniden runlamaya "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
